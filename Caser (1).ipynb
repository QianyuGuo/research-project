{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Caser.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"0ZE1-4ULzMt6","colab_type":"code","outputId":"3db8118c-bcd6-40e1-f164-2cc2438d20ae","executionInfo":{"status":"ok","timestamp":1552195505679,"user_tz":-660,"elapsed":30967,"user":{"displayName":"Qianyu Guo","photoUrl":"","userId":"03435097776792169966"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"0F1qlLhJ1Y4C","colab_type":"text"},"cell_type":"markdown","source":["**# This Data_Loader file **"]},{"metadata":{"id":"8WlaUlcPkN7U","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","import numpy as np\n","from tensorflow.contrib import learn\n","\n","# This Data_Loader file is copied online\n","class Data_Loader:\n","    def __init__(self, options):\n","\n","        positive_data_file = options['dir_name']\n","        positive_examples = list(open(positive_data_file, \"r\").readlines())\n","        positive_examples = [s for s in positive_examples]\n","\n","\n","        max_document_length = max([len(x.split(\",\")) for x in positive_examples])\n","        vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n","        self.item = np.array(list(vocab_processor.fit_transform(positive_examples)))\n","        self.item_dict = vocab_processor.vocabulary_._mapping\n","\n","\n","    def load_generator_data(self, sample_size):\n","        text = self.text\n","        mod_size = len(text) - len(text)%sample_size\n","        text = text[0:mod_size]\n","        text = text.reshape(-1, sample_size)\n","        return text, self.vocab_indexed\n","\n","\n","    def string_to_indices(self, sentence, vocab):\n","        indices = [ vocab[s] for s in sentence.split(',') ]\n","        return indices\n","\n","    def inidices_to_string(self, sentence, vocab):\n","        id_ch = { vocab[ch] : ch for ch in vocab } \n","        sent = []\n","        for c in sentence:\n","            if id_ch[c] == 'eol':\n","                break\n","            sent += id_ch[c]\n","\n","        return \"\".join(sent)\n","\n","   \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J_RczJv0y-g_","colab_type":"text"},"cell_type":"markdown","source":["**utils**"]},{"metadata":{"id":"IC9Ir_lmkeGZ","colab_type":"code","outputId":"81d22eac-26a6-4b6b-c443-3300ed41547f","executionInfo":{"status":"ok","timestamp":1552195547956,"user_tz":-660,"elapsed":2805,"user":{"displayName":"Qianyu Guo","photoUrl":"","userId":"03435097776792169966"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","\n","def sample_top(a=[], top_k=10):\n","    idx = np.argsort(a)[::-1]\n","    idx = idx[:top_k]\n","    probs = a[idx]\n","    probs = probs / np.sum(probs)\n","    choice = np.random.choice(idx, p=probs)\n","    return choice\n","\n","# fajie\n","def sample_top_k(a=[], top_k=10):\n","    idx = np.argsort(a)[::-1]\n","    idx = idx[:top_k]\n","    # probs = a[idx]\n","    # probs = probs / np.sum(probs)\n","    # choice = np.random.choice(idx, p=probs)\n","    return idx\n","\n","print sample_top_k(np.array([0.02,0.01,0.01,0.16,0.8]),3)\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[4 3 0]\n"],"name":"stdout"}]},{"metadata":{"id":"cJNGlSdazEkC","colab_type":"text"},"cell_type":"markdown","source":["**text_cnn_hv.py**"]},{"metadata":{"id":"ISQf1dMrrBSN","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","'''\n","including verical and horizonal cnn\n","'''\n","class TextCNN_hv(object):\n","    \"\"\"\n","    A CNN for text classification.\n","    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n","    \"https://github.com/dennybritz/cnn-text-classification-tf\"\n","    \"\"\"\n","    def __init__(\n","      self, sequence_length, num_classes, vocab_size,\n","      embedding_size, filter_sizes, num_filters, loss_type, l2_reg_lambda):\n","\n","        # Placeholders for input, output and dropout\n","\n","\n","\n","        self.wholesession = tf.placeholder('int32',\n","                                         [None, None], name='wholesession')\n","\n","        # a=self.t_sentence.get_shape()[1]*2\n","\n","\n","        source_sess = self.wholesession[:, 0:-1]\n","        target_sess = self.wholesession[:, -1:]\n","\n","        new_sequence_length=sequence_length-1\n","\n","        # source_embedding = tf.nn.embedding_lookup(self.wholesession,\n","        #                                           source_sess, name=\"source_embedding\")\n","        # target_embedding=tf.nn.embedding_lookup(self.wholesession,\n","        #                                    target_sess, name=\"target_sess\")\n","\n","\n","        # self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n","        # self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n","        self.input_x=source_sess\n","        self.input_y=target_sess\n","\n","        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n","\n","        self.loss_type = loss_type\n","        self.l2_reg_lambda = l2_reg_lambda\n","\n","\n","        # Keeping track of l2 regularization loss (optional)\n","        l2_loss = tf.constant(0.0)\n","\n","        # Embedding layer\n","        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n","            self.W = tf.Variable(\n","                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n","                name=\"W\")\n","            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n","            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n","\n","        # Create a convolution + maxpool layer for each filter size\n","        pooled_outputs = []\n","        for i, filter_size in enumerate(filter_sizes):\n","            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                # Convolution Layer\n","                filter_shape = [filter_size, embedding_size, 1, num_filters]\n","                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n","                # http://www.infoq.com/cn/articles/introduction-of-tensorflow-part4   how to use cnn\n","                # new shape after conv2d[?, new_sequence_length - filter_size + 1, 1, 1]\n","                conv = tf.nn.conv2d(\n","                    self.embedded_chars_expanded,\n","                    W,\n","                    strides=[1, 1, 1, 1],\n","                    padding=\"VALID\",\n","                    name=\"conv\")\n","                # Apply nonlinearity\n","                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                # Maxpooling over the outputs\n","                # new shape after max_pool[?, 1, 1, num_filters]\n","                # be carefyul, the  new_sequence_length has changed because of wholesession[:, 0:-1]\n","                pooled = tf.nn.max_pool(\n","                    h,\n","                    ksize=[1, new_sequence_length - filter_size + 1, 1, 1],\n","                    strides=[1, 1, 1, 1],\n","                    padding='VALID',\n","                    name=\"pool\")\n","                pooled_outputs.append(pooled)\n","\n","        # Combine all the pooled features\n","        num_filters_total = num_filters * len(filter_sizes)\n","        self.h_pool = tf.concat(pooled_outputs, 3)\n","        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) #shape=[batch_size, 384]\n","        # design the veritcal cnn\n","        with tf.name_scope(\"conv-verical\" ):\n","            filter_shape = [new_sequence_length, 1, 1, 1]\n","            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","            b = tf.Variable(tf.constant(0.1, shape=[1]), name=\"b\")\n","            conv = tf.nn.conv2d(\n","                self.embedded_chars_expanded,\n","                W,\n","                strides=[1, 1, 1, 1],\n","                padding=\"VALID\",\n","                name=\"conv\")\n","            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","        self.vcnn_flat= tf.reshape(h, [-1, embedding_size])\n","        self.final=tf.concat([self.h_pool_flat,self.vcnn_flat],1) #shape=[batch_size, 384+100]\n","\n","\n","\n","\n","        # Add dropout\n","        with tf.name_scope(\"dropout\"):\n","            self.h_drop = tf.nn.dropout(self.final, self.dropout_keep_prob)\n","\n","        # Final (unnormalized) scores and predictions\n","        with tf.name_scope(\"output\"):\n","            W = tf.get_variable(\n","                \"W\",\n","                shape=[num_filters_total+embedding_size, num_classes],\n","                initializer=tf.contrib.layers.xavier_initializer())\n","            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n","            l2_loss += tf.nn.l2_loss(W)\n","            l2_loss += tf.nn.l2_loss(b)\n","            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n","            # self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n","            # losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n","            self.input_y = tf.reshape(self.input_y, [-1])\n","            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.scores)\n","            self.loss = tf.reduce_mean(self.loss + l2_reg_lambda * l2_loss)\n","\n","            self.probs_flat = tf.nn.softmax(self.scores)\n","            self.arg_max_prediction = tf.argmax(self.probs_flat, 1)\n","\n","\n","        # Calculate mean cross-entropy loss\n","        # with tf.name_scope(\"loss\"):\n","        #     if self.loss_type == 'square_loss':\n","        #         if self.l2_reg_lambda > 0:\n","        #             self.loss = tf.nn.l2_loss(\n","        #                 tf.subtract(self.input_y, self.scores)) +  l2_reg_lambda * l2_loss  # regulizer\n","        #         else:\n","        #             self.loss = tf.nn.l2_loss(tf.subtract(self.input_y, self.scores))\n","\n","\n","\n","\n","\n","\n","        # Accuracy\n","        # with tf.name_scope(\"accuracy\"):\n","        #     correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n","        #     self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eoFX7ZPr1bQq","colab_type":"text"},"cell_type":"markdown","source":["**参数类**"]},{"metadata":{"id":"6CG1aRbp2t2e","colab_type":"code","colab":{}},"cell_type":"code","source":["class Args(object):\n","    \"\"\"docstring for Hotel\"\"\"\n","    def __init__(self,\n","                 learning_rate = 0.001,\n","                 batch_size=10, \n","                 sample_every=2000,\n","                 summary_every=50,\n","                 save_model_every = 1500,\n","                 sample_size=300,\n","                 top_k = 5,\n","                 max_epochs = 50,\n","                 beta1 = 0.5,\n","                 resume_model = None,\n","                 text_dir ='/content/gdrive/My Drive/test.csv' ,\n","                 data_dir = '/content/gdrive/My Drive/',\n","                 seed='f78c95a8-9256-4757-9a9f-213df5c6854e,1151b040-8022-4965-96d2-8a4605ce456c',\n","                 sample_percentage = 0.2,\n","                 filter_sizes = '[2,3,4]',\n","                 num_filters = 100,#128\n","                 loss_type = 'square_loss',#'Specify a loss type (square_loss or log_loss).'\n","                 l2_reg_lambda = 0,\n","                 allow_soft_placement = True,\n","                 log_device_placement = False,\n","                 dropout_keep_prob =0.5\n","                 \n","                ):\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.sample_every = sample_every\n","        self.summary_every = summary_every\n","        self.save_model_every = save_model_every\n","        self.sample_size = sample_size\n","        self.top_k = top_k\n","        self.max_epochs = max_epochs\n","        self.beta1 =beta1\n","        self.resume_model = resume_model\n","        self.text_dir=text_dir\n","        self.data_dir=data_dir\n","        self.seed=seed\n","        self.sample_percentage=sample_percentage\n","        self.filter_sizes=filter_sizes\n","        self.num_filters=num_filters\n","        self.loss_type=loss_type\n","        self.l2_reg_lambda=l2_reg_lambda\n","        self.allow_soft_placement=allow_soft_placement\n","        self.log_device_placement=log_device_placement\n","        self.dropout_keep_prob=dropout_keep_prob\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"32arDAnb1cVf","colab_type":"text"},"cell_type":"markdown","source":["**caser**"]},{"metadata":{"id":"nfKH5zyqrJVb","colab_type":"code","outputId":"ca89aa3c-6e49-42cf-855c-f909bb163fd3","executionInfo":{"status":"ok","timestamp":1552195944403,"user_tz":-660,"elapsed":71936,"user":{"displayName":"Qianyu Guo","photoUrl":"","userId":"03435097776792169966"}},"colab":{"base_uri":"https://localhost:8080/","height":1887}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import argparse\n","import shutil\n","import time\n","import os\n","import sys\n","import math\n","\n","\n","'''\n","reimplementation of\n","Personalized Top-N Sequential Recommendation via\n","Convolutional Sequence Embedding\n","screen print has been changed a bit so that to print the output not that ofen\n","'''\n","def main():\n","   \n","    args = Args()\n","\n","\n","\n","    dl = Data_Loader({'model_type': 'generator', 'dir_name': args.text_dir})\n","    # text_samples=16390600  vocab=947255  session100\n","\n","    all_samples = dl.item\n","    items = dl.item_dict\n","\n","\n","    # Randomly shuffle data\n","    np.random.seed(10)\n","   \n","    shuffle_indices = np.random.permutation(np.arange(len(all_samples)))\n","    text_samples = all_samples[shuffle_indices]\n","\n","\n","\n","    # Split train/test set\n","    # TODO: This is very crude, should use cross-validation\n","    dev_sample_index = -1 * int(args.sample_percentage * float(len(text_samples)))\n","    x_train, x_dev = text_samples[:dev_sample_index], text_samples[dev_sample_index:]\n","\n","    #create subsession only for training\n","    subseqtrain = []\n","    for i in range(len(x_train)):\n","        #print x_train[i]\n","        seq=x_train[i]\n","        lenseq=len(seq)\n","        #session lens=100 shortest subsession=5 realvalue+95 0\n","        for j in range(lenseq-4):\n","            subseqend=seq[:len(seq)- j]\n","            subseqbeg=[0]*j\n","            subseq= np.append(subseqbeg,subseqend)\n","            #beginseq=padzero+subseq\n","            #newsubseq=pad+subseq\n","            subseqtrain.append(subseq)\n","    x_train=np.array(subseqtrain) #list to ndarray\n","    del subseqtrain\n","    # Randomly shuffle data\n","    np.random.seed(10)\n","    shuffle_train = np.random.permutation(np.arange(len(x_train)))\n","    x_train = x_train[shuffle_train]\n","    print \"generating subsessions is done!\"\n","    print \"shape\", x_train.shape[0]\n","    print \"dataset\",args.text_dir\n","    model_options = {\n","        'vocab_size': len(items),\n","        'residual_channels': 100,\n","    }\n","\n","\n","    cnn = TextCNN_hv(\n","        sequence_length=x_train.shape[1],\n","        num_classes=len(items),\n","        vocab_size=len(items),\n","        embedding_size= model_options['residual_channels'],\n","        filter_sizes=eval(args.filter_sizes),\n","        num_filters=args.num_filters,\n","        loss_type=args.loss_type,\n","        l2_reg_lambda=args.l2_reg_lambda)\n","\n","    print \"embedding_size\", model_options['residual_channels']\n","    session_conf = tf.ConfigProto(\n","        # allow to distribute device automatically if your assigned device is not found\n","        allow_soft_placement=args.allow_soft_placement,\n","        # whether print or not\n","        log_device_placement=args.log_device_placement)\n","    sess = tf.Session(config=session_conf)\n","    with sess.as_default():\n","        # Define Training procedure\n","        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n","        optimizer = tf.train.AdamOptimizer(1e-3)\n","        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n","        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n","        sess.run(tf.global_variables_initializer())\n","\n","\n","\n","\n","\n","    step = 1\n","    for epoch in range(args.max_epochs):\n","        batch_no = 0\n","        batch_size = args.batch_size\n","        while (batch_no + 1) * batch_size < x_train.shape[0]:\n","\n","            start = time.clock()\n","\n","            text_batch = x_train[batch_no * batch_size: (batch_no + 1) * batch_size, :]\n","\n","            _, loss, prediction = sess.run(\n","                [train_op, cnn.loss,\n","                 cnn.arg_max_prediction],\n","                feed_dict={\n","                    cnn.wholesession: text_batch,\n","                    cnn.dropout_keep_prob: args.dropout_keep_prob\n","                })\n","            end = time.clock()\n","            if step % args.sample_every == 0:\n","                print \"-------------------------------------------------------train1\"\n","                print \"LOSS: {}\\tEPOCH: {}\\tBATCH_NO: {}\\t STEP:{}\\t total_batches:{}\".format(\n","                    loss, epoch, batch_no, step, x_train.shape[0] / args.batch_size)\n","                print \"TIME FOR BATCH\", end - start\n","                print \"TIME FOR EPOCH (mins)\", (end - start) * (x_train.shape[0] / args.batch_size) / 60.0\n","\n","            # print \"-------------------------------------------------------train2\"\n","            # loss = sess.run(\n","            #     [generator_model.loss_test],\n","            #     feed_dict={\n","            #         generator_model.seed_sentence: text_batch\n","            #     })\n","            # print \"LOSS: {}\\tEPOCH: {}\\tBATCH_NO: {}\\t STEP:{}\\t total_batches:{}\".format(\n","            #     loss, epoch, batch_no, step, x_train.shape[0] / args.batch_size)\n","\n","            # print \"-------------------------------------------------------test1\"\n","            # if (batch_no + 1) * batch_size < x_dev.shape[0]:\n","            #     text_batch = x_dev[(batch_no) * batch_size: (batch_no + 1) * batch_size, :]\n","            # loss = sess.run(\n","            #     [generator_model.loss],\n","            #     feed_dict={\n","            #         generator_model.t_sentence: text_batch\n","            #     })\n","            # print \"LOSS: {}\\tEPOCH: {}\\tBATCH_NO: {}\\t STEP:{}\\t total_batches:{}\".format(\n","            #     loss, epoch, batch_no, step, x_dev.shape[0] / args.batch_size)\n","            if step % args.sample_every == 0:\n","                print \"-------------------------------------------------------test1\"\n","                if (batch_no + 1) * batch_size < x_dev.shape[0]:\n","                    text_batch = x_dev[(batch_no) * batch_size: (batch_no + 1) * batch_size, :]\n","                loss = sess.run(\n","                    [cnn.loss],\n","                    feed_dict={\n","                        cnn.wholesession: text_batch,\n","                        cnn.dropout_keep_prob: 1.0\n","                    })\n","                print \"LOSS: {}\\tEPOCH: {}\\tBATCH_NO: {}\\t STEP:{}\\t total_batches:{}\".format(\n","                    loss, epoch, batch_no, step, x_dev.shape[0] / args.batch_size)\n","\n","\n","\n","            batch_no += 1\n","\n","            if step % args.sample_every == 0:\n","                print \"********************************************************accuracy\"\n","                batch_no_test = 0\n","                batch_size_test = batch_size*2\n","                curr_preds_5 = []\n","                rec_preds_5 = []  # 1\n","                ndcg_preds_5 = []  # 1\n","                curr_preds_20 = []\n","                rec_preds_20 = []  # 1\n","                ndcg_preds_20 = []  # 1\n","                while (batch_no_test + 1) * batch_size_test < x_dev.shape[0]:\n","                    # do not need to evaluate all, only after several 10 sample_every, then output final results\n","                    if(step / (args.sample_every)<10):\n","                        if (batch_no_test > 2):\n","                            break\n","                    else:\n","                        if (batch_no_test > 500):\n","                            break\n","                    text_batch = x_dev[batch_no_test * batch_size_test: (batch_no_test + 1) * batch_size_test, :]\n","                    [probs] = sess.run(\n","                        [cnn.probs_flat],\n","                        feed_dict={\n","                            cnn.wholesession: text_batch,\n","                            cnn.dropout_keep_prob: 1.0\n","                        })\n","                    for bi in range(probs.shape[0]):\n","                        pred_words_5 = sample_top_k(probs[bi], top_k=args.top_k)  # top_k=5\n","                        pred_words_20 = sample_top_k(probs[bi], top_k=args.top_k + 15)\n","\n","                        true_word = text_batch[bi][-1]\n","                        predictmap_5 = {ch: i for i, ch in enumerate(pred_words_5)}\n","                        pred_words_20 = {ch: i for i, ch in enumerate(pred_words_20)}\n","\n","                        rank_5 = predictmap_5.get(true_word)\n","                        rank_20 = pred_words_20.get(true_word)\n","                        if rank_5 == None:\n","                            curr_preds_5.append(0.0)\n","                            rec_preds_5.append(0.0)  # 2\n","                            ndcg_preds_5.append(0.0)  # 2\n","                        else:\n","                            MRR_5 = 1.0 / (rank_5 + 1)\n","                            Rec_5 = 1.0  # 3\n","                            ndcg_5 = 1.0 / math.log(rank_5 + 2, 2)  # 3\n","                            curr_preds_5.append(MRR_5)\n","                            rec_preds_5.append(Rec_5)  # 4\n","                            ndcg_preds_5.append(ndcg_5)  # 4\n","                        if rank_20 == None:\n","                            curr_preds_20.append(0.0)\n","                            rec_preds_20.append(0.0)  # 2\n","                            ndcg_preds_20.append(0.0)  # 2\n","                        else:\n","                            MRR_20 = 1.0 / (rank_20 + 1)\n","                            Rec_20 = 1.0  # 3\n","                            ndcg_20 = 1.0 / math.log(rank_20 + 2, 2)  # 3\n","                            curr_preds_20.append(MRR_20)\n","                            rec_preds_20.append(Rec_20)  # 4\n","                            ndcg_preds_20.append(ndcg_20)  # 4\n","\n","                    batch_no_test += 1\n","                    print \"BATCH_NO: {}\".format(batch_no_test)\n","                    print \"Accuracy mrr_5:\", sum(curr_preds_5) / float(len(curr_preds_5))  # 5\n","                    print \"Accuracy mrr_20:\", sum(curr_preds_20) / float(len(curr_preds_20))  # 5\n","                    print \"Accuracy hit_5:\", sum(rec_preds_5) / float(len(rec_preds_5))  # 5\n","                    print \"Accuracy hit_20:\", sum(rec_preds_20) / float(len(rec_preds_20))  # 5\n","                    print \"Accuracy ndcg_5:\", sum(ndcg_preds_5) / float(len(ndcg_preds_5))  # 5\n","                    print \"Accuracy ndcg_20:\", sum(ndcg_preds_20) / float(len(ndcg_preds_20))  #\n","                    # print \"curr_preds\",curr_preds\n","\n","            step += 1\n","\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-1-da24523f08f0>:19: __init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tensorflow/transform or tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: __init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tensorflow/transform or tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tensorflow/transform or tf.data.\n","generating subsessions is done!\n","shape 1444\n","dataset /content/gdrive/My Drive/test.csv\n","WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-3-dd3d3f9205f3>:113: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","embedding_size 100\n","WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","-------------------------------------------------------train1\n","LOSS: 0.0860433652997\tEPOCH: 13\tBATCH_NO: 127\t STEP:2000\t total_batches:144\n","TIME FOR BATCH 0.009735\n","TIME FOR EPOCH (mins) 0.023364\n","-------------------------------------------------------test1\n","LOSS: [0.017618757]\tEPOCH: 13\tBATCH_NO: 127\t STEP:2000\t total_batches:36\n","********************************************************accuracy\n","BATCH_NO: 1\n","Accuracy mrr_5: 0.1\n","Accuracy mrr_20: 0.102631578947\n","Accuracy hit_5: 0.1\n","Accuracy hit_20: 0.15\n","Accuracy ndcg_5: 0.1\n","Accuracy ndcg_20: 0.111568910658\n","BATCH_NO: 2\n","Accuracy mrr_5: 0.15\n","Accuracy mrr_20: 0.151315789474\n","Accuracy hit_5: 0.15\n","Accuracy hit_20: 0.175\n","Accuracy ndcg_5: 0.15\n","Accuracy ndcg_20: 0.155784455329\n","BATCH_NO: 3\n","Accuracy mrr_5: 0.175\n","Accuracy mrr_20: 0.17865497076\n","Accuracy hit_5: 0.183333333333\n","Accuracy hit_20: 0.216666666667\n","Accuracy ndcg_5: 0.17718216256\n","Accuracy ndcg_20: 0.186975252564\n","-------------------------------------------------------train1\n","LOSS: 0.0049966564402\tEPOCH: 27\tBATCH_NO: 111\t STEP:4000\t total_batches:144\n","TIME FOR BATCH 0.009089\n","TIME FOR EPOCH (mins) 0.0218136\n","-------------------------------------------------------test1\n","LOSS: [0.00014591489]\tEPOCH: 27\tBATCH_NO: 111\t STEP:4000\t total_batches:36\n","********************************************************accuracy\n","BATCH_NO: 1\n","Accuracy mrr_5: 0.1\n","Accuracy mrr_20: 0.107142857143\n","Accuracy hit_5: 0.1\n","Accuracy hit_20: 0.15\n","Accuracy ndcg_5: 0.1\n","Accuracy ndcg_20: 0.116666666667\n","BATCH_NO: 2\n","Accuracy mrr_5: 0.1375\n","Accuracy mrr_20: 0.142738095238\n","Accuracy hit_5: 0.15\n","Accuracy hit_20: 0.2\n","Accuracy ndcg_5: 0.140773243839\n","Accuracy ndcg_20: 0.155356577173\n","BATCH_NO: 3\n","Accuracy mrr_5: 0.175\n","Accuracy mrr_20: 0.178492063492\n","Accuracy hit_5: 0.2\n","Accuracy hit_20: 0.233333333333\n","Accuracy ndcg_5: 0.181546487679\n","Accuracy ndcg_20: 0.191268709901\n","-------------------------------------------------------train1\n","LOSS: 0.000894052907825\tEPOCH: 41\tBATCH_NO: 95\t STEP:6000\t total_batches:144\n","TIME FOR BATCH 0.009377\n","TIME FOR EPOCH (mins) 0.0225048\n","-------------------------------------------------------test1\n","LOSS: [7.841628e-05]\tEPOCH: 41\tBATCH_NO: 95\t STEP:6000\t total_batches:36\n","********************************************************accuracy\n","BATCH_NO: 1\n","Accuracy mrr_5: 0.1\n","Accuracy mrr_20: 0.105555555556\n","Accuracy hit_5: 0.1\n","Accuracy hit_20: 0.15\n","Accuracy ndcg_5: 0.1\n","Accuracy ndcg_20: 0.115051499783\n","BATCH_NO: 2\n","Accuracy mrr_5: 0.15\n","Accuracy mrr_20: 0.152777777778\n","Accuracy hit_5: 0.15\n","Accuracy hit_20: 0.175\n","Accuracy ndcg_5: 0.15\n","Accuracy ndcg_20: 0.157525749892\n","BATCH_NO: 3\n","Accuracy mrr_5: 0.179166666667\n","Accuracy mrr_20: 0.181018518519\n","Accuracy hit_5: 0.2\n","Accuracy hit_20: 0.216666666667\n","Accuracy ndcg_5: 0.184360105194\n","Accuracy ndcg_20: 0.189377271788\n"],"name":"stdout"}]},{"metadata":{"id":"quUdNrPvEXFL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}